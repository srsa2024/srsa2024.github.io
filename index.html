<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content=".">
  <meta name="keywords" content="Robotic assembly tasks; Reinforcement learning finetuning.">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>SRSA: Skill Retrieval and Adaptation for Robotics Assembly Tasks</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>
   -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/robotic-arm.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
   <script type="text/x-mathjax-config">
     MathJax.Hub.Config({
       tex2jax: {
         inlineMath: [ ['$','$'], ["\\(","\\)"] ],
         processEscapes: true
       }
     });
   </script>
   <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">SRSA: Skill Retrieval and Adaptation for Robotics Assembly Tasks</h1>
	  <h2 class="subtitle is-2 conference-title" style="color:blue;"> ICLR 2025 Spotlight </h2>
	  <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://research.nvidia.com/person/yijie-guo">Yijie Guo</a><sup>1</sup>,
	    </span>
            <span class="author-block">
              <a href="https://bingjietang718.github.io">Bingjie Tang</a><sup>2</sup>,
	    </span>
            <span class="author-block">
              <a href="https://research.nvidia.com/person/iretiayo-akinola">Iretiayo Akinola</a><sup>1</sup>,
	    </span>
	    <span class="author-block">
              <a href="https://research.nvidia.com/person/dieter-fox">Dieter Fox</a><sup>1,3</sup>,
            </span>
	    <span class="author-block">
              <a href="https://homes.cs.washington.edu/~abhgupta/">Abhishek Gupta</a><sup>1,3</sup>,
            </span>
            <span class="author-block">
              <a href="https://research.nvidia.com/person/yashraj-narang">Yashraj Narang</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>NVIDIA Corporation,</span>
            <span class="author-block"><sup>2</sup>University of Southern California,</span>
	    <span class="author-block"><sup>3</sup>University of Washington</span>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- <embed src="static/images/teaser.pdf" style="width:100%;" frameborder="0"></embed> -->

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/overview.png" width="100%">
      <!-- h4 class="subtitle has-text-centered">
        <b>Overview of SRSA</b>. In this work, we propose to predict the transfer success of applying prior skills to new tasks and retrieve the skill with the highest transfer success prediction. 
	</h4> -->
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Enabling robots to learn novel tasks in a data-efficient manner is a long-standing challenge. Common strategies involve carefully leveraging prior experiences, especially transition data collected on related tasks. Although much progress has been made for general pick-and-place manipulation, far fewer studies have investigated contact-rich assembly tasks, where precise control is essential.
	  </p>
          <p>
            We introduce SRSA (Skill Retrieval and Skill Adaptation), a novel framework designed to address this problem by utilizing a pre-existing skill library containing policies for diverse assembly tasks. The challenge lies in identifying which skill from the library is most relevant for fine-tuning on a new task. Our key hypothesis is that skills showing higher zero-shot success rates on a new task are better suited for rapid and effective fine-tuning on that task. To this end, we propose to predict the transfer success for all skills in the skill library on a novel task, and then use this prediction to guide the skill retrieval process. We establish a framework that jointly captures features of object geometry, physical dynamics, and expert actions
to represent the tasks, allowing us to efficiently learn the transfer success predictor. 
	  </p>
	  <p>
	    Extensive experiments demonstrate that SRSA significantly outperforms the leading baseline. When retrieving and fine-tuning skills on unseen tasks, SRSA achieves a 19% relative improvement in success rate, exhibits 2.6x lower standard deviation across random seeds, and requires 2.4x fewer transition samples to reach a satisfactory success rate, compared to the baseline. In a continual learning setup, SRSA efficiently learns policies for new tasks and incorporates them into the skill library, enhancing future policy learning. Furthermore, policies trained with SRSA in simulation achieve a 90% mean success rate when deployed in the real world.
	  </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
    
    <!-- Paper video. -->
    <div style="padding:56.25% 0 0 0;position:relative;"><iframe src="https://player.vimeo.com/video/1027794699?badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479" frameborder="0" allow="autoplay; fullscreen; picture-in-picture; clipboard-write" style="position:absolute;top:0;left:0;width:100%;height:100%;" title="srsa_video_2"></iframe></div>
    <!--/ Paper video. -->
    <h2 class="subtitle has-text-centered">
        5-min Overview Video with Narration.
    </h2>
  </div>
</section>

<!--section class="section" id="overview">
  <div class="container is-max-desktop content">
    Paper video.
    <h2 class="title">SRSA Overview</h2>
      <p>
        We briefly introduce SRSA method, experiments and results in the 5-min video below.
      </p>
      <div style="padding:56.25% 0 0 0;position:relative;"><iframe src="https://player.vimeo.com/video/1027794699?badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479" frameborder="0" allow="autoplay; fullscreen; picture-in-picture; clipboard-write" style="position:absolute;top:0;left:0;width:100%;height:100%;" title="srsa_video_2"></iframe></div>-->
      <!--<div style="padding:56.25% 0 0 0;position:relative;"><iframe src="https://player.vimeo.com/video/1022199337?badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479" frameborder="0" allow="autoplay; fullscreen; picture-in-picture; clipboard-write" style="position:absolute;top:0;left:0;width:100%;height:100%;" title="srsa_video (1)"></iframe></div>-->
    <!--<div style="padding:56.25% 0 0 0;position:relative;"><iframe src="https://player.vimeo.com/video/1019922368?badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479" frameborder="0" allow="autoplay; fullscreen; picture-in-picture; clipboard-write" style="position:absolute;top:0;left:0;width:100%;height:100%;" title="srsa_video"></iframe></div>-->
    <!--/ Paper video. -->
  <!--/div>
  </section>-->

<section class="section" id="Problem">
  <div class="container is-max-desktop content">
    <h2 class="title">Problem Setup</h2>
    <div class="content has-text-justified">
      <p>
        In this work, we consider the problem setting of solving a new target task leveraging pre-existing skills from a skill library. We focus on two-part assembly tasks as shown below. 
      </p>
      <img src="static/images/assembly_tasks.png" width="100%">

      <p>
      Given a target task, we assume access to a prior task set. The skill library contains policies that solve each of the prior tasks, respectively. To solve a target task, the goal of reinforcement learning is to find a policy that produces an action for each state to maximize the expected return. We propose to first <b>retrieve a skill (i.e., policy) for the most relevant prior task</b>, and then <b>rapidly and effectively adapt to the target task by fine-tuning the retrieved skill</b>.
      </p>
    </div>
  </div>
</section>

<section class="section" id="Specialist">
  <div class="container is-max-desktop content">
    <h2 class="title">Skill Retrieval</h2>
    <div class="content has-text-justified">
      <p>
        To effectively retrieve the skills that are useful for a new target task T , we require a means to estimate the potential of applying a source policy to the target task. We are inspired by two points from theoretical perspective:
        <ul>
          <li>Applying the same policy on two tasks, similar success rate imply similarity in dynamics and initial state distributions.</li>
          <li>Fine-tuning a source policy on a target task with similar dynamics to the source task could be efficient.</li>
        </ul>
      Therefore, we propose using zero-shot transfer success as a metric to gauge the potential to efficiently adapt a source policy to a target task. To identify a source policy with high zero-shot transfer success on a given target task, we propose to learn <b>a function $F$ to predict zero-shot transfer success</b> for any pair of source policy and target task.
      </p>

      <h4 class="title">1. Dataset Formulation to Learn Transfer Success Predictor</h4>
      <p>
         We treat any two tasks from the prior task set as a source-target task pair. For each pair, we evaluate the source policy on the target task to obtain the zero-shot transfer success rate. The transfer success predictor takes the information about source and target tasks as input, and output the zero-shot transfer success.
      </p>

      <h4 class="title">2. Learning Task Features for Transfer Success Predictor</h4>
      <p>We need a strong featurization of both the source and target tasks for efficient learning of transfer success predictor. We propose a framework that jointly captures features of geometry, dynamics, and expert actions to represent the tasks.
        <ul>
          <li><b>(a)</b> Geometry features are learned from point-cloud input using a PointNet autoencoder. </li>
          <li><b>(b)</b> Dynamics features are learned from transition segments
using a state-prediction objective. </li>
          <li><b>(c)</b> Expert-action features are learned from transition segments
using an action-reconstruction objective. </li>
        </ul>  
      </p>
      <center><img src="static/images/embedding.png" width="70%"></center><br>

      <h4 class="title">3. Training Transfer Success Predictor</h4>
      <p>
       The geometry, dynamics, and expert action features are concatenated together to form task features. We then pass the concatenated task features through an MLP to predict the transfer success. 
      </p>
      <center><img src="static/images/transfer_pred.png" width="60%"></center><br>

      <h4 class="title">4. Inferring Transfer Success for Retrieval</h4>
      <p>
       At test time, we use the well-trained transfer success predictor to predict the transfer success of applying any prior policy to a new task. We retrieve the source policy with a high predicted transfer success.
      </p>
      <p>
        We compare our approach (SRSA) with the baseline retrieval strategies (Signature, Behavior, Forward, Geometry), as shown in the figure below. Overall, SRSA retrieves source policies that obtain around 10% higher success rates on the test tasks.
      </p>
      <img src="static/images/retrieval.png" width="100%"><br>
      
    </div>
  </div>
</section>

<section class="section" id="Generalist">
  <div class="container is-max-desktop content">
    <h2 class="title">Skill Adaptation</h2>
    <div class="content has-text-justified">
      <p>
      Our ultimate goal is to solve the new task as an RL problem. The retrieved skill is used to initialize the policy network. We subsequently use proximal policy optimization (PPO) and <a href="https://arxiv.org/abs/1806.05635">self-imitation learning</a> to fine-tune the policy on the target task. We compare SRSA with the leading baseline <a href="https://bingjietang718.github.io/automate/">AutoMate</a> to learn specialist policies for two-part assembly tasks. We consider two settings:
        <ul>
          <li>Dense-reward setting: includes a reward term imitating disassembly demonstrations and a curriculum.</li>
          <li>Sparse-reward setting: only provides a nonzero reward for task success to emulate real-world RL fine-tuning, where dense-reward information is much more challenging to acquire. </li>
        </ul>
        The learning curves in the set of test tasks show that SRSA achieves strong performance with fewer training epochs and greater stability.
      </p>
      <img src="static/images/adaptation.png" width="100%"><br>
    </div>
  </div>
</section>


<section class="section" id="real demo">
  <div class="container is-max-desktop content">
    <h2 class="title">SRSA Policy Deployment in Simulation</h2>
      <p>
        We deploy the trained SRSA policies in simulation on 5 distinct assemblies and we show the policy performance in the videos below.
      </p>
    <div id="videoal">
      <table> 
        <tr>
          <td><center>01029</center></td>
          <td><center>01129</center></td>
          <td><center>01136</center></td>
          <td><center>01053</center></td>
          <td><center>01079</center></td>
        </tr>
        <tr>
          <td><img src="static/images/01029.png" width="100%"></td>
          <td><img src="static/images/01129.png" width="100%"></td>
          <td><img src="static/images/01136.png" width="100%"></td>
          <td><img src="static/images/01053.png" width="100%"></td>
          <td><img src="static/images/01079.png" width="100%"></td>
        </tr>
        <tr>
          <td><img src="static/images/01029.gif" width="100%"></td>
          <td><img src="static/images/01129.gif" width="100%"></td>
          <td><img src="static/images/01136.gif" width="100%"></td>
          <td><img src="static/images/01053.gif" width="100%"></td>
          <td><img src="static/images/01079.gif" width="100%"></td>
        </tr>
      </table>
    </div>
  </div>
</section>



<section class="section" id="real demo">
  <div class="container is-max-desktop content">
    <h2 class="title">SRSA vs. AutoMate in the Real World</h2>
      <p>
        We deploy the trained SRSA policies in the real world and directly compare them to <a href="https://bingjietang718.github.io/automate/">AutoMate</a>.
      </p>
      <p> 
        The experimental setup is the same as AutoMate, we place the robot in lead-through (a.k.a., manual guide mode), grasp a plug, guide it into the socket, and record the pose as a target pose. We then programmatically lift the plug until free from contact; apply an $xy$ perturbation of $ \pm 10$ mm, $z$ perturbation of $15 \pm 5$ mm, and yaw perturbation of $\pm 5^{\circ}$; apply $x$, $y$, and $z$ observation noise of $\pm 2$ mm each; and deploy a policy. 
      </p>
      <p> 
        We show the comparison between SRSA policies and AutoMate policies on 5 distinct assemblies in the videos below. 
    <div id="videoal">
      <table>
          <tr>
            <iframe src="https://player.vimeo.com/video/1014143493?title=0&amp;byline=0&amp;portrait=0&amp;badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479" frameborder="0" allow="autoplay; fullscreen; picture-in-picture; clipboard-write" width="100%"></iframe>
          </tr>
          <tr>
            <iframe src="https://player.vimeo.com/video/1014143222?title=0&amp;byline=0&amp;portrait=0&amp;badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479" frameborder="0" allow="autoplay; fullscreen; picture-in-picture; clipboard-write" width="100%"></iframe>
          </tr>
          <tr>
            <iframe src="https://player.vimeo.com/video/1014143285?title=0&amp;byline=0&amp;portrait=0&amp;badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479" frameborder="0" allow="autoplay; fullscreen; picture-in-picture; clipboard-write" width="100%"></iframe>
          </tr>
          <tr>
            <iframe src="https://player.vimeo.com/video/1014143172?title=0&amp;byline=0&amp;portrait=0&amp;badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479" frameborder="0" allow="autoplay; fullscreen; picture-in-picture; clipboard-write" width="100%"></iframe>
          </tr>
          <tr>
            <iframe src="https://player.vimeo.com/video/1014143104?h=bce8b7a76c&amp;title=0&amp;byline=0&amp;portrait=0&amp;badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479" frameborder="0" allow="autoplay; fullscreen; picture-in-picture; clipboard-write" width="100%"></iframe>
          </tr>
      </table>
    </div>
  </div>
</section>


<section class="section" id="real demo">                                                                                  <div class="container is-max-desktop content">
    <h2 class="title">Frequently Asked Questions</h2>
    <ul>
      <li>
	<b>Q</b>: How do you initialize the critic network when fine-tuning a policy on a new target task?
      </li>
      <p>
        <b>A</b>: All source policies are initially trained with dense rewards, following the AutoMate approach for specialist policies. When fine-tuning on a target task with dense rewards, we initialize the critic network using the weights of the source policy's critic, as the reward scale is consistent with the one used in the original training. However, if we fine-tune with sparse rewards, we randomly initialize the critic network to accommodate the different reward scale and structure.
      </p>
      <li>
	<b>Q</b>: Why does sparse reward sometimes outperform dense reward, for example, in task 1053?
      </li>
      <p>
        <b>A</b>: Task 1053 is relatively simple since both the plug and socket are large and easy to manipulate. In the dense reward setting, following the AutoMate framework, there’s an imitation reward component that encourages the agent to follow a disassembly trajectory. However, this disassembly trajectory may introduce bias, potentially creating local minima or artifacts in the dense reward function. For simpler tasks like this one, sparse rewards that only reward success can provide a more direct and unbiased signal, leading to better performance.
      </p>
      <li>
	<b>Q</b>: To retrieve the best policy, rather than training a transfer success predictor, you could instead (at inference time) evaluate all source policies in the library on the target task to get the zero-shot transfer success rate. What are the limitations of this naive approach for retrieval?
      </li>
      <p>
        <b>A</b>: This approach is indeed a straightforward method for skill retrieval, but it’s computationally intensive. Evaluating a single source policy on a target task requires 1,000 episodes to reliably measure the zero-shot transfer success rate, which takes about 20 minutes on a single V100 GPU. As the skill library grows, this process becomes increasingly costly and unscalable, as evaluating every source policy on each target task to identify the most relevant one would require significant computational resources.
      </p>
      <li>
	<b>Q</b>: How do you handle storage for a growing skill library?
      </li>
      <p>
        <b>A</b>: To manage storage as the skill library expands, we propose maintaining a fixed number of policies that maximize coverage of the task embedding space. Specifically, we propose clustering tasks based on a similarity matrix, where similarity is measured by the zero-shot transfer success rate between any pair of source policies and target tasks. For each cluster, we can retain a single policy capable of addressing all tasks within that cluster.
      </p>
    </ul>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            Website template borrowed from
              <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
