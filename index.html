<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content=".">
  <meta name="keywords" content="Robotic assembly tasks; Reinforcement learning finetuning.">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>SRSA: Skill Retrieval and Adaptation for Robotics Assembly Tasks</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>
   -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/robotic-arm.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
   <script type="text/x-mathjax-config">
     MathJax.Hub.Config({
       tex2jax: {
         inlineMath: [ ['$','$'], ["\\(","\\)"] ],
         processEscapes: true
       }
     });
   </script>
   <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">SRSA: Skill Retrieval and Adaptation for Robotics Assembly Tasks</h1>
	  <h2 class="subtitle is-2 conference-title" style="color:blue;"> ICLR 2025 Spotlight </h2>
	  <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://research.nvidia.com/person/yijie-guo">Yijie Guo</a><sup>1</sup>,
	    </span>
            <span class="author-block">
              <a href="https://bingjietang718.github.io">Bingjie Tang</a><sup>2</sup>,
	    </span>
            <span class="author-block">
              <a href="https://research.nvidia.com/person/iretiayo-akinola">Iretiayo Akinola</a><sup>1</sup>,
	    </span>
	    <span class="author-block">
              <a href="https://research.nvidia.com/person/dieter-fox">Dieter Fox</a><sup>1,3</sup>,
            </span>
	    <span class="author-block">
              <a href="https://homes.cs.washington.edu/~abhgupta/">Abhishek Gupta</a><sup>1,3</sup>,
            </span>
            <span class="author-block">
              <a href="https://research.nvidia.com/person/yashraj-narang">Yashraj Narang</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>NVIDIA Corporation,</span>
            <span class="author-block"><sup>2</sup>University of Southern California,</span>
	    <span class="author-block"><sup>3</sup>University of Washington</span>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- <embed src="static/images/teaser.pdf" style="width:100%;" frameborder="0"></embed> -->

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/teaser.jpg" width="100%">
      <h4 class="subtitle has-text-centered">
        <b>Overview of SRSA</b>. In this work, we propose to predict the transfer success of applying prior skills to new tasks and retrieve the skill with the highest transfer success prediction. 
      </h4>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Enabling robots to learn novel tasks in a data-efficient manner remains a significant challenge. A common approach to address this issue is leveraging prior experiences, such as sharing multi-task policies on new tasks or learning from transition data in related tasks. While much progress has been made in retrieving relevant prior experience for general pick-and-place manipulation tasks, the application of these methods to contact-rich manipulation tasks remains underexplored, particularly for assembly tasks that require precise control in industries. 
          </p>
          <p>
            In this work, we investigate the problem of utilizing a pre-existing skill library containing diverse policies for various assembly tasks. We introduce SRSA (Skill Retrieval and Skill Adaptation), a novel framework that retrieves relevant skills from the library and adapts them to efficiently solve new robotic assembly tasks. Our approach involves first predicting transfer success to guide skill retrieval, followed by fine-tuning the retrieved policy with self-imitation learning. We demonstrate that SRSA effectively selects policies with high success rates on new tasks and SRSA learns policies on new tasks with improved performance, stability, and sample efficiency compared to baseline methods, particularly in sparse-reward scenarios. Additionally, SRSA shows promise in continually expanding the skill library across diverse tasks. It provides an efficient way to collect a large set of specialist policies, which support real-world deployment for robotic assembly tasks.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

  </div>
</section>

<section class="section" id="overview">
  <div class="container is-max-desktop content">
    <!-- Paper video. -->
    <h2 class="title">SRSA Overview</h2>
      <p>
        We briefly introduce SRSA method, experiments and results in the 5-min video below.
      </p>
    <div style="padding:56.25% 0 0 0;position:relative;"><iframe src="https://player.vimeo.com/video/1027794699?badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479" frameborder="0" allow="autoplay; fullscreen; picture-in-picture; clipboard-write" style="position:absolute;top:0;left:0;width:100%;height:100%;" title="srsa_video_2"></iframe></div>
    <!--<div style="padding:56.25% 0 0 0;position:relative;"><iframe src="https://player.vimeo.com/video/1022199337?badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479" frameborder="0" allow="autoplay; fullscreen; picture-in-picture; clipboard-write" style="position:absolute;top:0;left:0;width:100%;height:100%;" title="srsa_video (1)"></iframe></div>-->
    <!--<div style="padding:56.25% 0 0 0;position:relative;"><iframe src="https://player.vimeo.com/video/1019922368?badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479" frameborder="0" allow="autoplay; fullscreen; picture-in-picture; clipboard-write" style="position:absolute;top:0;left:0;width:100%;height:100%;" title="srsa_video"></iframe></div>-->
    <!--/ Paper video. -->
  </div>
</section>

<section class="section" id="real demo">
  <div class="container is-max-desktop content">
    <h2 class="title">SRSA Policy Deployment in Simulation</h2>
      <p>
        We deploy the trained policies in simulation on 5 distinct assemblies and we show the policy performance in the videos below.
      </p>
    <div id="videoal">
      <table> 
        <tr>
          <td><center>01029</center></td>
          <td><center>01129</center></td>
          <td><center>01136</center></td>
          <td><center>01053</center></td>
          <td><center>01079</center></td>
        </tr>
        <tr>
          <td><img src="static/images/01029.png" width="100%"></td>
          <td><img src="static/images/01129.png" width="100%"></td>
          <td><img src="static/images/01136.png" width="100%"></td>
          <td><img src="static/images/01053.png" width="100%"></td>
          <td><img src="static/images/01079.png" width="100%"></td>
        </tr>
        <tr>
          <td><img src="static/images/01029.gif" width="100%"></td>
          <td><img src="static/images/01129.gif" width="100%"></td>
          <td><img src="static/images/01136.gif" width="100%"></td>
          <td><img src="static/images/01053.gif" width="100%"></td>
          <td><img src="static/images/01079.gif" width="100%"></td>
        </tr>
      </table>
    </div>
  </div>
</section>



<section class="section" id="real demo">
  <div class="container is-max-desktop content">
    <h2 class="title">SRSA vs. AutoMate in the Real World</h2>
      <p>
        We deploy the trained SRSA policies in the real world and directly compare them to <a href="https://bingjietang718.github.io/automate/">AutoMate</a>.
      </p>
      <p> 
        The experimental setup is the same as AutoMate, we place the robot in lead-through (a.k.a., manual guide mode), grasp a plug, guide it into the socket, and record the pose as a target pose. We then programmatically lift the plug until free from contact; apply an $xy$ perturbation of $ \pm 10$ mm, $z$ perturbation of $15 \pm 5$ mm, and yaw perturbation of $\pm 5^{\circ}$; apply $x$, $y$, and $z$ observation noise of $\pm 2$ mm each; and deploy a policy. 
      </p>
      <p> 
        We show the comparison between SRSA policies and AutoMate policies on 5 distinct assemblies in the videos below. 
    <div id="videoal">
      <table>
          <tr>
            <iframe src="https://player.vimeo.com/video/1014143493?title=0&amp;byline=0&amp;portrait=0&amp;badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479" frameborder="0" allow="autoplay; fullscreen; picture-in-picture; clipboard-write" width="100%"></iframe>
          </tr>
          <tr>
            <iframe src="https://player.vimeo.com/video/1014143222?title=0&amp;byline=0&amp;portrait=0&amp;badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479" frameborder="0" allow="autoplay; fullscreen; picture-in-picture; clipboard-write" width="100%"></iframe>
          </tr>
          <tr>
            <iframe src="https://player.vimeo.com/video/1014143285?title=0&amp;byline=0&amp;portrait=0&amp;badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479" frameborder="0" allow="autoplay; fullscreen; picture-in-picture; clipboard-write" width="100%"></iframe>
          </tr>
          <tr>
            <iframe src="https://player.vimeo.com/video/1014143172?title=0&amp;byline=0&amp;portrait=0&amp;badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479" frameborder="0" allow="autoplay; fullscreen; picture-in-picture; clipboard-write" width="100%"></iframe>
          </tr>
          <tr>
            <iframe src="https://player.vimeo.com/video/1014143104?h=bce8b7a76c&amp;title=0&amp;byline=0&amp;portrait=0&amp;badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479" frameborder="0" allow="autoplay; fullscreen; picture-in-picture; clipboard-write" width="100%"></iframe>
          </tr>
      </table>
    </div>
  </div>
</section>


<section class="section" id="real demo">                                                                                  <div class="container is-max-desktop content">
    <h2 class="title">Frequently Asked Questions</h2>
    <ul>
      <li>
	<b>Q</b>: How do you initialize the critic network when fine-tuning a policy on a new target task?
      </li>
      <p>
        <b>A</b>: All source policies are initially trained with dense rewards, following the AutoMate approach for specialist policies. When fine-tuning on a target task with dense rewards, we initialize the critic network using the weights of the source policy's critic, as the reward scale is consistent with the one used in the original training. However, if we fine-tune with sparse rewards, we randomly initialize the critic network to accommodate the different reward scale and structure.
      </p>
      <li>
	<b>Q</b>: Why does sparse reward sometimes outperform dense reward, for example, in task 1053?
      </li>
      <p>
        <b>A</b>: Task 1053 is relatively simple since both the plug and socket are large and easy to manipulate. In the dense reward setting, following the AutoMate framework, there’s an imitation reward component that encourages the agent to follow a disassembly trajectory. However, this disassembly trajectory may introduce bias, potentially creating local minima or artifacts in the dense reward function. For simpler tasks like this one, sparse rewards that only reward success can provide a more direct and unbiased signal, leading to better performance.
      </p>
      <li>
	<b>Q</b>: To retrieve the best policy, rather than training a transfer success predictor, you could instead (at inference time) evaluate all source policies in the library on the target task to get the zero-shot transfer success rate. What are the limitations of this naive approach for retrieval?
      </li>
      <p>
        <b>A</b>: This approach is indeed a straightforward method for skill retrieval, but it’s computationally intensive. Evaluating a single source policy on a target task requires 1,000 episodes to reliably measure the zero-shot transfer success rate, which takes about 20 minutes on a single V100 GPU. As the skill library grows, this process becomes increasingly costly and unscalable, as evaluating every source policy on each target task to identify the most relevant one would require significant computational resources.
      </p>
      <li>
	<b>Q</b>: How do you handle storage for a growing skill library?
      </li>
      <p>
        <b>A</b>: To manage storage as the skill library expands, we propose maintaining a fixed number of policies that maximize coverage of the task embedding space. Specifically, we propose clustering tasks based on a similarity matrix, where similarity is measured by the zero-shot transfer success rate between any pair of source policies and target tasks. For each cluster, we can retain a single policy capable of addressing all tasks within that cluster.
      </p>
    </ul>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            Website template borrowed from
              <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
